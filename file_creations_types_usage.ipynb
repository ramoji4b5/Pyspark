{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acce528e-5bab-4b38-8e16-974170628401",
   "metadata": {},
   "source": [
    "# How to create various files using Pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e528e88-2203-4081-993f-4f0991e81887",
   "metadata": {},
   "source": [
    "--format\n",
    "1.csv\n",
    "2. json\n",
    "3. Parquet\n",
    "4. Avro\n",
    "5. Excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7002ec8-cde5-4ea0-b131-0be76a0717b5",
   "metadata": {},
   "source": [
    "# 1. CSV File Operations in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9225e2-96ed-4c71-8929-424d37a086c5",
   "metadata": {},
   "source": [
    "1.1 Create / Write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1656b9fe-3029-4771-9a49-81b40b7af46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ca2a3c-7e2e-4699-8593-f9ea7122211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark.sql.functions as F\n",
    "#import pyspark.sql.types as T\n",
    "#from pyspark.sql import Window\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c479a773-cf87-4aa8-9d08-0c6d16185727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Ramoji-Rao-Yalamati:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f341184810>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74dcc490-5db2-488a-b8c6-c0d6832f1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('D:\\CODING\\PySpark\\employe_task1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286f789f-b231-4cb8-878c-b351b2a880e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+--------------+\n",
      "|_c0|                 _c1|         _c2|           _c3|\n",
      "+---+--------------------+------------+--------------+\n",
      "| id|                none|actualSalary|expectedSalary|\n",
      "|  1|   created by Ramoji|        3000|          5000|\n",
      "|  2|submited by Laksh...|       10000|         15000|\n",
      "|  3|   approved by Swami|       10000|         12000|\n",
      "+---+--------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d95a7f-77ad-4430-81bb-4d6a860499c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"D:/CODING/PySpark/employe_task1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9431e67e-a6dc-4d6d-90a6-7b533185323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+--------------+\n",
      "|_c0|                 _c1|         _c2|           _c3|\n",
      "+---+--------------------+------------+--------------+\n",
      "| id|                none|actualSalary|expectedSalary|\n",
      "|  1|   created by Ramoji|        3000|          5000|\n",
      "|  2|submited by Laksh...|       10000|         15000|\n",
      "|  3|   approved by Swami|       10000|         12000|\n",
      "+---+--------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c7a5ed-a145-4c20-9430-593e22c61248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|order_id|customer|amount|\n",
      "+--------+--------+------+\n",
      "|     101|    John|   250|\n",
      "|     102|    Mary|   300|\n",
      "|     103|    Alex|   150|\n",
      "+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.csv(\"D:/CODING/PySpark/sales.csv\", header=True, inferSchema=True)\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60bfef4e-6152-45ed-baf6-fe7da3e504f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.write.mode(\"overwrite\").csv(\"pyspark/sales_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb81ea07-e2a5-4015-b551-5c4da9e773c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_json = spark.read.json(\"D:/CODING/PySpark/employee.json\")\n",
    "# df_json.printSchema()\n",
    "# df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "003dbc55-4b87-4f5f-b288-ad9a0fbe2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e285f-fa78-4fd4-98ac-9c169185e55b",
   "metadata": {},
   "source": [
    "## JSON FIle Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a85ff6c3-ef12-4b4a-9727-51d04e46682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|age| id|name|\n",
      "+---+---+----+\n",
      "| 30|  1|John|\n",
      "| 25|  2|Rita|\n",
      "+---+---+----+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadJSON\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"D:/CODING/PySpark/employee.json\")\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "108d03f5-8bc4-40a0-a1d3-3035fcb68496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+---------------+\n",
      "|address            |id |name|skills         |\n",
      "+-------------------+---+----+---------------+\n",
      "|{Hyderabad, 500001}|1  |Alex|[python, spark]|\n",
      "+-------------------+---+----+---------------+\n",
      "\n",
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- pincode: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").json(r\"D:\\CODING\\PySpark\\employee_details.json\")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8fc264b-fe33-4627-896d-d3dd24b015c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|id |name|\n",
      "+---+----+\n",
      "|1  |John|\n",
      "|2  |Ravi|\n",
      "+---+----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\", True).json(r\"D:\\CODING\\PySpark\\multi_line.json\")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f37270-1a36-4fc6-b4a6-e019b19cedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f764f04-7524-497d-93b6-719697d9cc29",
   "metadata": {},
   "source": [
    "## Parquet File Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b0d3193-0ab5-4ffd-a4fb-d6ba70164ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "|data|   id|                  ts|\n",
      "+----+-----+--------------------+\n",
      "| XYZ|10102|2019-04-23 23:50:...|\n",
      "| ABC|10101|2019-04-23 22:55:...|\n",
      "| XYZ|10102|2019-04-23 23:50:...|\n",
      "| ABC|10101|2019-04-23 22:55:...|\n",
      "+----+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- data: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadParquet\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"data2.parquet\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d61cf51e-27d6-4689-82a4-b6169b0c8540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|salary|\n",
      "+---+----+------+\n",
      "|  1|John|  1000|\n",
      "|  2|Mary|  2000|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write Parquet\n",
    "data = [(1,\"John\",1000), (2,\"Mary\",2000)]\n",
    "cols = [\"id\",\"name\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.write.mode(\"overwrite\").parquet(\"parquet_examples\")\n",
    "\n",
    "# Read it\n",
    "df2 = spark.read.parquet(\"parquet_examples\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c0fc4f1-f178-4736-bb18-dac204cefc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e6c4a-a234-4b5c-bc42-2fae788ac480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
